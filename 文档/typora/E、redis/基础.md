# 一、特性

1. 非关系型的键值对数据库，可以根据键以O(1)的时间复杂度查找或插入关联值

   > 整个key-value是hash表存储

2. Redis的数据是存在内存中的，性能高

3. 键的类型是唯一的，可以是字符串、整形、浮点型

4. 值的类似可以是：string、list、set、sorted set、hash

5. Redis内置了复制、磁盘持久化、LUA脚本、事务、客户端代理等功能

6. 通过哨兵和自动分区提供高可用性

7. 单线程每秒数十万级别的处理能力

## 1. 快速的redis有哪些慢操作

1. redis的hashtable扩容时，为了避免阻塞客户端请求，采用渐进式rehash，
   并且有两个hashtable，查询时先查新的，查不到在查旧的；插入在新的hash表，更新删除新旧都操作
2. 范围操作：返回集合的全部数据或者部分数据：HGETALL、SMEMBERS、LRANGE、ZRANGE，
   应尽量避免，可以使用SCAN改进

性能瓶颈：

 1. 单个请求耗时，阻塞了后面的请求

    a. 操作bigkey：插入分配内存、删除释放内存都十分耗时

    b. 使用复杂度过高的命令：事件复杂度O(n)

    c. 大量key集中过期：Redis的过期机制是在主线程中执行的

    d. 淘汰策略：也是在主线程上执行的

    e. AOF刷盘使用always机制：每次写入都需要把该操作刷到磁盘

    f. 主从全量同步生成RDB：虽然采用fork子进程生成数据快照，但fork这一瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久

    g.主从同步：从库需要**清空当前数据库**，还需要将RDB文件加载到内存中，RDB文件越大，加载就越慢

 2. 并发量非常大

针对问题1，一方面需要业务人员去规避，一方面Redis在4.0推出了lazy-free机制，把bigkey释放内存的耗时操作放在了异步线程中执行，降低对主线程的影响。

针对问题2，Redis在6.0推出了多线程，可以在高并发场景下利用CPU多核多线程读写客户端数据，进一步提升server性能，当然，只是针对客户端的读写是并行的，每个命令的真正操作依旧是单线程的。

## 2. 高性能的IO模型--多路复用模型

>Redis单线程：主要是指网络IO和键值对读写是一个线程完成的。
>                   持久化、异步删除、集群数据同步是由额外的线程执行的。  

多线程问题：共享资源的并发访问+线程的上下文切换

**Redis单线程快的原因：**

​	<strong style="color:red">内存读写+高效的数据结构+多路复用IO模型</strong>

**多路复用IO模型：**

​    并发处理大量的客户端请求，提升吞吐量

### 2.1 单线程处理网络IO的问题

<img src="D:\myself\springboot-example\文档\typora\images\redis10.jpg" alt="img" style="zoom: 25%;" />



阻塞点：

	1. accept：Redis监听到一个连接请求，但一直未能成功建立连接
 	2. recv：Redis通过recv从一个客户端读取数据，如果数据一直无法到达

   以上两点都会导致Redis线程阻塞，无法处理其他客户端请求

 解决：使用Socket网络模型：支持非阻塞模式

### 2.2 非阻塞模式

在 socket 模型中，不同操作调用后会返回不同的套接字类型。socket() 方法会返回主动套接字，然后调用 listen() 方法，将主动套接字转化为监听套接字，此时，可以监听来自客户端的连接请求。最后，调用 accept() 方法接收到达的客户端连接，并返回已连接套接字

<img src="D:\myself\springboot-example\文档\typora\images\redis11.jpg" alt="img" style="zoom: 25%;" />

调用accept和recv方法时，使用监听套接字监听，当连接请求或数据未到达时，Redis线程可以去处理其他事情，无需等待
当数据到达时，通过某种机制去通知Redis处理，这时，我们可以用到Linux的IO多路复用机制

### 2.3 基于多路复用的高性能I/O模型

Linux 中的 IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的 select/epoll 机制

在 Redis 只运行单线程的情况下，该机制允许内核中，同时存在多个监听套接字和已连接套接字

<img src="D:\myself\springboot-example\文档\typora\images\redis12.jpg" alt="img" style="zoom:25%;" />

Redis调用epoll，让内核监听这些套接字（FD）,一旦套接字上有请求达到时，根据事件调用相应的回调函数

## 3. 持久化

原因：数据存在内存中，一旦服务器宕机，内存数据将全部丢失

为什么不从数据库中同步：

1. 需要频繁访问数据库，给数据库带来巨大的压力

 	2. 读取数据库性能低

### 3.1 AOF

记录的是命令而不是数据

例如: 执行 set testkey testvalue，保存的是：

```
*3         当前命令有三个部分
$3         命令有几个字节
set
$7
testkey
$9
testvalue
```

**写回策略：**

1. **always**：每次写操作同步写入磁盘

 	2. **EverySec**：每秒写回，每个写命令执行完，先吧日志写到AOF的内存缓冲区，每隔1秒把日志写入磁盘
 	3. **NO**：操作系统写回，每个写命令执行完，先吧日志写到AOF的内存缓冲区，由操作系统决定何时写入磁盘

主线程阻塞和数据丢失风险，无法做到两全其美

<img src="D:\myself\springboot-example\文档\typora\images\redis13.jpg" alt="img" style="zoom:33%;" />

<img src="D:\myself\springboot-example\文档\typora\images\redis21.jpg" alt="img" style="zoom:33%;" />

当主线程使用后台子线程执行了一次 fsync，需要再次把新接收的操作记录写回磁盘时，如果主线程发现上一次的 fsync 还没有执行完，那么它就会阻塞。所以，如果后台子线程执行的 fsync 频繁阻塞的话（比如 AOF 重写占用了大量的磁盘 IO 带宽），主线程也会阻塞，导致 Redis 性能变慢

随着命令的执行，AOF日志文件会越来越大，性能也会受到影响：

1. 磁盘文件的大小本身收到限制

 	2. 文件过大，每次写入性能降低
 	3. 服务宕机，恢复过程缓慢



**AOF日志重写机制：**

> AOF 重写机制就是在重写时，Redis 根据数据库的现状创建一个新的 AOF 文件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入

​    <strong style="color:red">AOF重写会阻塞主线程吗</strong>

​    和AOF文件是由主线程写入不同，重写过程是后台子进程`berewriteaof`完成的。

**一个拷贝，两处日志**

“一个拷贝”就是指，每次执行重写时，主线程 fork 出后台的 bgrewriteaof 子进程。此时，fork 会把主线程的内存拷贝一份给 bgrewriteaof 子进程，这里面就包含了数据库的最新数据。然后，bgrewriteaof 子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。

“两处日志”：因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指正在使用的 **AOF 日志**，Redis 会把这个操作写到它的缓冲区。这样一来，即使宕机了，这个 AOF 日志的操作仍然是齐全的，可以用于恢复

而第二处日志，就是指新的 **AOF 重写日志**。这个操作也会被写到重写日志的缓冲区。这样，重写日志也不会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我们就可以用新的 AOF 文件替代旧文件了。

​     <strong style="color:red">AOF fork子进程风险</strong>

1. fork这个瞬间一定是阻塞主线程的，fork不会一次性拷贝所有内存数据给子进程，而是采用操作系统提供的**写时复制（Copy-On-Write**）就是为了避免一次性拷贝大量内存数据给子进程造成的长时间阻塞

   但fork子进程需要拷贝进程必要的数据结构，其中有一项就是**拷贝内存页表**（虚拟内存和物理内存的映射索引表），这个拷贝过程会消耗大量CPU资源，拷贝完成之前整个进程是会阻塞的，阻塞时间取决于整个实例的内存大小，**实例越大，内存页表越大，fork阻塞时间越久**。拷贝内存页表完成后，子进程与父进程指向相同的内存地址空间，也就是说此时虽然产生了子进程，但是并没有申请与父进程相同的内存大小。那什么时候父子进程才会真正内存分离呢？“写实复制”顾名思义，就是在写发生时，才真正拷贝内存真正的数据，这个过程中，父进程也可能会产生阻塞的风险，就是下面介绍的场景。

2. fork出的子进程指向与父进程相同的内存地址空间，此时子进程就可以执行AOF重写，把内存中的所有数据写入到AOF文件中。

   此时主线程仍然有数据写入，如果父进程操作的是一个已经存在的key，那么这个时候父进程就会真正拷贝这个key对应的内存数据，申请新的内存空间，这样逐渐地，父子进程内存数据开始分离，父子进程逐渐拥有各自独立的内存空间。

   因为内存分配是以页为单位进行分配的，默认4k，如果父进程此时操作的是一个**bigkey**，**重新申请大块内存耗时会变长**，可能会产阻塞风险。

   如果操作系统开启了**内存大页机制**(Huge Page，页面大小2M)，那么父进程申请内存时阻塞的概率将会大大提高，所以在Redis机器上需要关闭Huge Page机制。

   Redis每次fork生成RDB或AOF重写完成后，都可以在Redis log中看到父进程重新申请了多大的内存空间。

**如何触发AOF重写**：

 1. 手动执行命令：`bgrewriteaof`

 2. 通过配置文件自动触发

    ```
    1. auto-aof-rewrite-min-size: 表示运行AOF重写时文件的最小大小，默认为64MB
    2. auto-aof-rewrite-percentage: 这个值的计算方法是：当前AOF文件大小和上一次重写后AOF文件大小的差值，再除以上一次重写后AOF文件大小。也就是当前AOF文件比上一次重写后AOF文件的增量大小，和上一次重写后AOF文件大小的比值。
    
    AOF文件大小同时超出上面这两个配置项时，会触发AOF重写。
    ```

    

**优缺点：**

   优点：每次操作只需记录命令，需要持久化的数据量不大

   缺点：进行故障恢复时，需要逐一把操作执行一遍。如果操作日志非常多，Redis 就会恢复得很缓慢，影响到正常使用。

### 3.2 RDB

> **内存快照**：记录的是内存某一时刻的数据，在做故障恢复的时候，直接把数据读入内存即可，

<strong style="color:red">做快照时，数据还能被增删改吗？(主线程会被阻塞吗)</strong>

   Redis提供两个命令来生成RDB文件：

		1. **save**：在主线程中执行，会导致阻塞；
  		2. **bgsave**：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置。

使用bgsave生成rdb文件时，采用了**写时复制**，避免阻塞写请求

>与AOF一样，fork拷贝内存页表，主线程读操作不受影响，当有写操作时，并且key存在快照中时，主线程拷贝一份数据到新的内存，并把这个内存地址置为主线程的虚拟内存地址执向它，这样，父子进程内存真正分离，主线程的写请求也不会受到影响。

**快照生成的频率**

​	频繁地执行全量快照，也会带来两方面的开销。

    1. 频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环
       2. bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了

**增量快照：**

> **Redis 4.0 中提出了一个混合使用 AOF 日志和内存快照**

**如何触发RDB**

1. shutdown(正常关闭)时，如果没有开启aof，会触发；

2. redis.conf默认配置(此配置是针对bgsave)

   根据这个默认配置，会丢数据；意外宕机的情况下，丢失最后一次持久化后的数据

   ```mysql
   save 900 1  #900s检查一次，增量的数据变更命令超过1，就触发；
   save 300 10 #300s检查一次 更改10次
   sava 60 10000 #60s检查一次  更改命令1w条，就触发；
   ```

3. 执行命令save或者bgsave

4. 执行**flushall命令**，清空rdb(Redis默认16个库都清空)；但是里面是空的，无意义

**关于 AOF 和 RDB 的选择问题的建议**：

1. 数据不能丢失时，内存快照和 AOF 的混合使用是一个很好的选择；
2. 如果允许分钟级别的数据丢失，可以只使用 RDB；
3. 如果只用 AOF，优先使用 everysec 的配置选项，因为它在可靠性和性能之间取了一个平衡

## 4. 主从同步

如果 Redis 发生了宕机，它们可以分别通过回放日志和重新读入 RDB 文件的方式恢复数据，从而保证尽量**少丢失数据**，提升可靠性

> Redis高可靠性：
>
> 	1. 数据尽量少丢失： AOF/RDB
>
> 2. 服务尽量少中断： 主从复制：数据保存到多个实例中

Redis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式

​	读操作：主库、从库都可以接收；
​	写操作：首先到主库执行，然后，主库将写操作同步给从库。

### 4.1 主从库如何进行第一次同步 

```C
replicaof 主库ip 主库端口
```

<img src="D:\myself\springboot-example\文档\typora\images\redis14.jpg" alt="img" style="zoom:50%;" />

**1. 主从库间建立连接、协商同步的过程，主要是为全量复制做准备**

从库发送psync ？ -1 命令，主库根据命令启动复制

```
psync 主库runId 复制进度offset
runId 是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实例。当从库和主库第一次复制时，因为不知道主库的 runID，所以将 runID 设为“？”
offset 此时设为 -1，表示第一次复制
```

主库收到 psync 命令后，会用 `FULLRESYNC` 响应命令带上两个参数：主库 runID 和主库目前的复制进度 offset，返回给从库。从库收到响应后，会记录下这两个参数

`FULLRESYNC` 响应表示第一次复制采用的**全量复制**，也就是说，主库会把当前所有的数据都复制给从库

**2. 主库发送所有数据给从库**

主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。从库接收到 RDB 文件后，会先清空当前数据库，然后加载 RDB 文件

**在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求**

为了保证主从库的数据一致性，主库会在内存中用专门的 replication buffer，记录 RDB 文件生成后收到的所有写操作

**3. 主库将新接收的写命令同步给从库**

当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了

### 4.2 主从级联模式分担主库全量复制时的压力

我们在部署主从集群的时候，可以手动选择一个从库（比如选择内存资源配置较高的从库），用于级联其他的从库。然后，我们可以再选择一些从库（例如三分之一的从库），在这些从库上执行如下命令，让它们和刚才所选的从库，建立起主从关系

```
replicaof 所选从库的IP 6379
```

<img src="D:\myself\springboot-example\文档\typora\images\redis15.jpg" alt="img" style="zoom:50%;" />



### 4.3 网络断链导致数据不一致

如果网络断连，主从库之间就无法进行命令传播了，从库的数据自然也就没办法和主库保持一致了，客户端就可能从从库读到旧数据

从 Redis 2.8 开始，网络断了之后，主从库会采用增量复制的方式继续同步

> **replication buffer**  每个客户端有一个
> 	client连上Redis后，Redis都会分配一个client buffer，所有数据交互都是通过这个buffer进行的
> 	Redis先把数据写到这个buffer中，然后再把buffer中的数据发到client socket中再通过网络发送出去，这样就完成了数据交互
>     主从库断开连接后，这个client buffer也消失了
>
> **repl_backlog_buffer** 环形 共享资源
>
> 它是为了从库断开之后，如何找到主从差异数据而设计的环形缓冲区，从而避免全量同步带来的性能开销。如果从库断开时间太久，repl_backlog_buffer环形缓冲区被主库的写命令覆盖了，那么从库连上主库后只能乖乖地进行一次全量同步，所以repl_backlog_buffer配置尽量大一些，可以降低主从断开后全量同步的概率。而在repl_backlog_buffer中找主从差异的数据后，如何发给从库呢？这就用到了replication buffer

主从库增量复制：
	主库接收到写命令，写入 replication buffer，同时也会把这些操作命令也写入 repl_backlog_buffer 这个缓冲区

  ![img](D:\myself\springboot-example\文档\typora\images\redis16.jpg)

master_repl_offset： 主库写的偏移量
slave_repl_offset： 从库读的偏移量，从库会保存

从库在复制完写操作命令后，它在缓冲区中的读位置也开始逐步偏移刚才的起始位置，此时，从库已复制的偏移量 slave_repl_offset 也在不断增加。正常情况下，这两个偏移量基本相等

主从库断开连接后，新的写入命令写入repl_backlog_buffer 

主从库的连接恢复之后，从库首先会给主库发送 psync 命令，并把自己当前的 slave_repl_offset 发给主库，主库会判断自己的 master_repl_offset 和 slave_repl_offset 之间的差距

**如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致**

因此，我们要想办法避免这一情况，一般而言，我们可以调整 repl_backlog_size 这个参数。这个参数和所需的缓冲空间大小有关。缓冲空间的计算公式是：缓冲空间大小 = 主库写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小。在实际应用中，考虑到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即 repl_backlog_size = 缓冲空间大小 * 2，这也就是 repl_backlog_size 的最终值

### 4.4 建议

全量复制虽然耗时，但是对于从库来说，如果是第一次同步，全量复制是无法避免的，所以，我给你一个小建议：一个 Redis 实例的数据库不要太大，一个实例大小在几 GB 级别比较合适，这样可以减少 RDB 文件生成、传输和重新加载的开销。另外，为了避免多个从库同时和主库进行全量复制，给主库过大的同步压力，我们也可以采用“主 - 从 - 从”这一级联模式，来缓解主库的压力

### 4.5 问题： 主从库间的复制不使用 AOF 呢？

RDB文件内容是经过压缩的二进制数据（不同数据类型数据做了针对性优化），文件很小。而AOF文件记录的是每一次写操作的命令，写操作越多文件会变得很大，其中还包括很多对同一个key的多次冗余操作。在主从全量数据同步时，传输RDB文件可以尽量降低对主库机器网络带宽的消耗，从库在加载RDB文件时，一是文件小，读取整个文件的速度会很快，二是因为RDB文件存储的都是二进制数据，从库直接按照RDB协议解析还原数据即可，速度会非常快，而AOF需要依次重放每个写命令，这个过程会经历冗长的处理逻辑，恢复速度相比RDB会慢得多，所以使用RDB进行主从全量同步的成本最低

假设要使用AOF做全量同步，意味着必须打开AOF功能，打开AOF就要选择文件刷盘的策略，选择不当会严重影响Redis性能。而RDB只有在需要定时备份和主从全量同步数据时才会触发生成一次快照。而在很多丢失数据不敏感的业务场景，其实是不需要开启AOF的

## 5. 哨兵机制

如果主库挂了，我们就需要运行一个新主库，比如说把一个从库切换为主库，把它当成主库。这就涉及到三个问题：

1. 主库真的挂了吗？
2. 该选择哪个从库作为主库？
3. 怎么把新主库的相关信息通知给从库和客户端呢？

哨兵其实就是一个运行在特殊模式下的 Redis 进程，主从库实例运行的同时，它也在运行。

### 5.1 基本流程

哨兵主要负责的就是三个任务：**监控、选主（选择主库）和通知**。

**监控**

监控是指哨兵进程在运行时，周期性地给所有的主从库发送 PING 命令，检测它们是否仍然在线运行。如果从库没有在规定时间内响应哨兵的 PING 命令，哨兵就会把它标记为“下线状态”；同样，如果主库也没有在规定时间内响应哨兵的 PING 命令，哨兵就会判定主库下线，然后开始自动切换主库的流程

<img src="D:\myself\springboot-example\文档\typora\images\redis17.jpg" alt="img" style="zoom:33%;" />



**主观下线**：

​	**哨兵进程会使用 PING 命令检测它自己和主、从库的网络连接情况，用来判断实例的状态**

​    从库超时：直接标记为下线
​    主库超时：直接标记为下线，可能存在哨兵误判，切换主库过程开销大，需要减少误判
​				误判一般会发生在集群网络压力较大、网络拥塞，或者是主库本身压力较大的情况下

**客观下线：**

​	使用哨兵集群：少数服从多数的原则判断主库是否断开连接
   只有大多数的哨兵实例，都判断主库已经“主观下线”了，主库才会被标记为“客观下线”

### 5.2 如何选择新主库

**筛选+打分**

筛选：在选主时，**除了要检查从库的当前在线状态，还要判断它之前的网络连接状态**

具体怎么判断呢？你使用配置项 down-after-milliseconds * 10。其中，down-after-milliseconds 是我们认定主从库断连的最大连接超时时间。如果在 down-after-milliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连了。如果发生断连的次数超过了 10 次，就说明这个从库的网络状况不好，不适合作为新主库。

> **要保证所有哨兵实例的配置是一致的，尤其是主观下线的判断值 down-after-milliseconds**。我们曾经就踩过一个“坑”。当时，在我们的项目中，因为这个值在不同的哨兵实例上配置不一致，导致哨兵集群一直没有对有故障的主库形成共识，也就没有及时切换主库，最终的结果就是集群服务不稳定

打分

	1. 优先级最高的从库得分高  配置：`slave-priority`
 	2. 和旧主库同步程度最接近的从库得分高
 	3. ID 号小的从库得分高

### 5.3 问题

1. **主库下线了，如何判断从库的同步程度**

   master_repl_offset和slave_repl_offset都是单调递增的，通过对缓存环的大小取模获取真实的位置，所以只需判断大小就行

2. **哨兵在操作主从切换的过程中，客户端能否正常地进行请求操作**

   如果客户端使用了读写分离，那么读请求可以在从库上正常执行，不会受到影响。但是由于此时主库已经挂了，而且哨兵还没有选出新的主库，所以在这期间写请求会失败，失败持续的时间 = 哨兵切换主从的时间 + 客户端感知到新主库 的时间

   如果不想让业务感知到异常，客户端只能把写失败的请求先缓存起来或写入消息队列中间件中，等哨兵切换完主从后，再把这些写请求发给新的主库，但这种场景只适合对写入请求返回值不敏感的业务，而且还需要业务层做适配，另外主从切换时间过长，也会导致客户端或消息队列中间件缓存写请求过多，切换完成之后重放这些请求的时间变长

   哨兵检测主库多久没有响应就提升从库为新的主库，这个时间是可以配置的（down-after-milliseconds参数）。配置的时间越短，哨兵越敏感，哨兵集群认为主库在短时间内连不上就会发起主从切换，这种配置很可能因为网络拥塞但主库正常而发生不必要的切换，当然，当主库真正故障时，因为切换得及时，对业务的影响最小。如果配置的时间比较长，哨兵越保守，这种情况可以减少哨兵误判的概率，但是主库故障发生时，业务写失败的时间也会比较久，缓存写请求数据量越多。

3. **应用程序不感知服务的中断，还需要哨兵和客户端做些什么？**

   当哨兵完成主从切换后，客户端需要及时感知到主库发生了变更，然后把缓存的写请求写入到新库中，保证后续写请求不会再受到影响，具体做法如下

   哨兵提升一个从库为新主库后，哨兵会把新主库的地址写入自己实例的pubsub（switch-master）中。客户端需要订阅这个pubsub，当这个pubsub有数据时，客户端就能感知到主库发生变更，同时可以拿到最新的主库地址，然后把写请求写到这个新主库即可，这种机制属于**哨兵主动通知客户端**。

   如果客户端因为某些原因错过了哨兵的通知，或者哨兵通知后客户端处理失败了，安全起见**，客户端也需要支持主动去获取最新主从的地址进行访问**

   所以，客户端需要访问主从库时，不能直接写死主从库的地址了，而是需要从哨兵集群中获取最新的地址（sentinel get-master-addr-by-name命令），这样当实例异常时，哨兵切换后或者客户端断开重连，都可以从哨兵集群中拿到最新的实例地址。

4. **哨兵集群中有实例挂了，怎么办，会影响主库状态判断和选主吗？**

   存在故障节点时，只要集群中大多数节点状态正常，集群依旧可以对外提供服务

5. **哨兵集群多数实例达成共识，判断出主库“客观下线”后，由哪个实例来执行主从切换呢**

   哨兵集群判断出主库“主观下线”后，会选出一个“哨兵领导者”，之后整个过程由它来完成主从切换

   简单来说就是每个哨兵设置一个随机超时时间，超时后每个哨兵会请求其他哨兵为自己投票，其他哨兵节点对收到的第一个请求进行投票确认，一轮投票下来后，首先达到多数选票的哨兵节点成为“哨兵领导者”，如果没有达到多数选票的哨兵节点，那么会重新选举，直到能够成功选出“哨兵领导者

## 6. 哨兵集群

```
# 配置哨兵 设置主库的 IP 和端口，并没有配置其他哨兵的连接信息
sentinel monitor <master-name> <ip> <redis-port> <quorum> 
```

### 6.1 基于pub/sub机制的哨兵集群组成

哨兵只要和主库建立起了连接，就可以在主库上发布消息了，比如说发布它自己的连接信息（IP 和端口）。同时，它也可以从主库上订阅消息，获得其他哨兵发布的连接信息。

在主从集群中，主库上有一个名为“__sentinel__:hello”的频道，不同哨兵就是通过它来相互发现，实现互相通信的。

哨兵订阅该频道，每个哨兵与主库建立连接时在该频道发布自己的地址信息，其他哨兵也就感知并可以建立连接

**哨兵是如何知道从库的 IP 地址和端口的呢？**

​	哨兵 2 给主库发送 INFO 命令，主库接受到这个命令后，就会把从库列表返回给哨兵

### 6.2 基于 pub/sub 机制的客户端事件通知

从本质上说，哨兵就是一个运行在特定模式下的 Redis 实例，只不过它并不服务请求操作，只是完成监控、选主和通知的任务。所以，每个哨兵实例也提供 pub/sub 机制，客户端可以从哨兵订阅消息。哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件

<img src="D:\myself\springboot-example\文档\typora\images\redis18.jpg" alt="img" style="zoom:33%;" />

### 6.3 由哪个哨兵执行主从切换？

**客观下线的具体流程：**

​	任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 is-master-down-by-addr 命令。接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票。

一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”。这个所需的赞成票数是通过哨兵配置文件中的 quorum 配置项设定的

此时，这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。这个投票过程称为“Leader 选举”。因为最终执行主从切换的哨兵称为 Leader，投票过程就是确定 Leader。

**Leader 选举**

​	任何一个想成为 Leader 的哨兵，要满足两个条件：

​		第一，拿到半数以上的赞成票；

​		第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。

<img src="D:\myself\springboot-example\文档\typora\images\redis19.jpg" alt="img" style="zoom:33%;" />

如果 S3 没有拿到 2 票 Y，那么这轮投票就不会产生 Leader。哨兵集群会等待一段时间（也就是哨兵故障转移超时时间的 2 倍），再重新选举。这是因为，哨兵集群能够进行成功投票，很大程度上依赖于选举命令的正常网络传播。如果网络压力较大或有短时堵塞，就可能导致没有一个哨兵能拿到半数以上的赞成票。所以，等到网络拥塞好转之后，再进行投票选举，成功的概率就会增加。

###  6.4 Redis 1主4从，5个哨兵，哨兵配置quorum为2，如果3个哨兵故障，当主库宕机时，哨兵能否判断主库“客观下线”？能否自动切换

1、哨兵集群可以判定主库“主观下线”。由于quorum=2，所以当一个哨兵判断主库“主观下线”后，询问另外一个哨兵后也会得到同样的结果，2个哨兵都判定“主观下线”，达到了quorum的值，因此，哨兵集群可以判定主库为“客观下线”

2、但哨兵不能完成主从切换。哨兵标记主库“客观下线后”，在选举“哨兵领导者”时，一个哨兵必须拿到超过多数的选票(5/2+1=3票)。但目前只有2个哨兵活着，无论怎么投票，一个哨兵最多只能拿到2票，永远无法达到多数选票的结果。

### 6.5 哨兵实例是不是越多越好？

我们也看到了，哨兵在判定“主观下线”和选举“哨兵领导者”时，都需要和其他节点进行通信，交换信息，哨兵实例越多，通信的次数也就越多，而且部署多个哨兵时，会分布在不同机器上，节点越多带来的机器故障风险也会越大，这些问题都会影响到哨兵的通信和选举，出问题时也就意味着选举时间会变长，切换主从的时间变久。

### 6.6 调大down-after-milliseconds值，对减少误判是不是有好处

是有好处的，适当调大down-after-milliseconds值，当哨兵与主库之间网络存在短时波动时，可以降低误判的概率。但是调大down-after-milliseconds值也意味着主从切换的时间会变长，对业务的影响时间越久，我们需要根据实际场景进行权衡，设置合理的阈值 

## 7. 切片集群

随着数据量的增长，Redis的响应越来越慢，这和Redis 的持久化机制有关系。在使用 RDB 进行持久化时，Redis 会 fork 子进程来完成，fork 操作的用时和 Redis 的数据量是正相关的，而 fork 在执行时会阻塞主线程。数据量越大，fork 操作造成的主线程阻塞的时间越长。所以，在使用 RDB 对 25GB 的数据进行持久化时，数据量较大，后台运行的子进程在 fork 创建时阻塞了主线程，于是就导致 Redis 响应变慢了

**纵向扩展**：升级单个 Redis 实例的资源配置，包括增加内存容量、增加磁盘容量、使用更高配置的 CPU。就像下图中，原来的实例内存是 8GB，硬盘是 50GB，纵向扩展后，内存增加到 24GB，磁盘增加到 150GB。
			纵向扩展会受到硬件和成本的限制

**横向扩展**：横向增加当前 Redis 实例的个数，就像下图中，原来使用 1 个 8GB 内存、50GB 磁盘的实例，现在使用三个相同配置的实例。

### 7.1 数据切片和实例的对应分布关系

Redis Cluster 方案采用哈希槽（Hash Slot，接下来我会直接称之为 Slot），来处理数据和实例之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。

根据键值对的 key，按照CRC16 算法计算一个 16 bit 的值；然后，再用这个 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽

自动分配：使用 cluster create 命令创建集群，此时，Redis 会自动把这些槽平均分布在集群实例上
手动分配：cluster addslots

**在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。**

### 7.2 客户端如何定位数据

客户端和集群实例建立连接后，实例就会把哈希槽的分配信息发给客户端

客户端收到哈希槽信息后，会把哈希槽信息缓存在本地。当客户端请求键值对时，会先计算键所对应的哈希槽，然后就可以给相应的实例发送请求了

在集群中，实例和哈希槽的对应关系并不是一成不变的，最常见的变化有两个：

- 在集群中，实例有新增或删除，Redis 需要重新分配哈希槽；
- 为了负载均衡，Redis 需要把哈希槽在所有实例上重新分布一遍

Redis Cluster 方案提供了一种重定向机制，所谓的“重定向”，就是指，客户端给一个实例发送数据读写操作时，这个实例上并没有相应的数据，客户端要再给一个新实例发送操作命令。

```
GET hello:key
(error) MOVED 13320 172.16.19.5:6379
```

当hash slot已经迁移后，客户端读取key时，返回moved 和该key新的hash slot对应的实例，客户端根据地址再次发送即可

但如果此时hash slot正在迁移，如key1和key2已经迁移，但key3还没有，此时客户端访问key2，返回ACK
客户端需要先给 172.16.19.5 这个实例发送一个 ASKING 命令。这个命令的意思是，让这个实例允许执行客户端接下来发送的命令。然后，客户端再向这个实例发送 GET 命令，以读取数据

```
GET key2
(error) ASK 13320 172.16.19.5:6379
```

### 7.3 Redis Cluster不采用把key直接映射到实例的方式，而采用哈希槽的方式原因：

1、整个集群存储key的数量是无法预估的，key的数量非常多时，直接记录每个key对应的实例映射关系，这个映射表会非常庞大，这个映射表无论是存储在服务端还是客户端都占用了非常大的内存空间。

2、Redis Cluster采用无中心化的模式（无proxy，客户端与服务端直连），客户端在某个节点访问一个key，如果这个key不在这个节点上，这个节点需要有纠正客户端路由到正确节点的能力（MOVED响应），这就需要节点之间互相交换路由表，每个节点拥有整个集群完整的路由关系。如果存储的都是key与实例的对应关系，节点之间交换信息也会变得非常庞大，消耗过多的网络资源，而且就算交换完成，相当于每个节点都需要额外存储其他节点的路由表，内存占用过大造成资源浪费。

3、当集群在扩容、缩容、数据均衡时，节点之间会发生数据迁移，迁移时需要修改每个key的映射关系，维护成本高。

4、而在中间增加一层哈希槽，可以把数据和节点解耦，key通过Hash计算，只需要关心映射到了哪个哈希槽，然后再通过哈希槽和节点的映射表找到节点，相当于消耗了很少的CPU资源，不但让数据分布更均匀，还可以让这个映射表变得很小，利于客户端和服务端保存，节点之间交换信息时也变得轻量。

5、当集群在扩容、缩容、数据均衡时，节点之间的操作例如数据迁移，都以哈希槽为基本单位进行操作，简化了节点扩容、缩容的难度，便于集群的维护和管理。

# 二、统计选取数据类型

## 2.1 聚合统计

当需要对多个集合做聚合统计时，推荐使用set。

> 所谓的聚合统计，就是指统计多个集合元素的聚合结果，包括：统计多个集合的共有元素（交集统计）；把两个集合相比，统计其中一个集合独有的元素（差集统计）；统计多个集合的所有元素（并集统计）

```
# user:id和user:id:0207的并集结果存到user:id
SUNIONSTORE user:id user:id user:id:0207 

# user:id和user:id:0207的差集结果存到user:new
SDIFFSTORE user:new user:id user:id:0207

# user:id和user:id:0207的交集结果存到user:id:rem
SINTERSTORE user:id:rem user:id user:id:0207
```

当集合比较大时，聚合操作将阻塞主线程。

Set数据类型，使用SUNIONSTORE、SDIFFSTORE、SINTERSTORE做并集、差集、交集时，选择一个从库进行聚合计算”。这3个命令都会在Redis中生成一个新key，而从库默认是readonly不可写的，所以这些命令只能在主库使用。想在从库上操作，可以使用**SUNION、SDIFF、SINTER**，这些命令可以计算出结果，但**不会生成新key**

改进：

	1. 指定一个从库做复杂的聚合运算
 	2. 读取数据到客户端进行聚合运算

**注意**

​		如果是在集群模式使用多个key聚合计算的命令，一定要注意，因为这些key可能分布在不同的实例上，多个实例之间是无法做聚合运算的，这样操作可能会直接报错或者得到的结果是错误的！

## 2.2 排序统计

面对需要展示最新列表、排行榜等场景时，如果数据更新频繁或者需要分页显示，建议你优先考虑使用 Sorted Set

## 2.3 二值统计

bitmap

## 2.4 基数统计

> 基数统计就是指统计一个集合中不重复的元素个数

set、hashtable随着集合元素的增多，占用的空间也越来越大

`HyperLogLog`**是一种用于统计基数的数据集合类型，它的最大优势就在于，当集合元素数量非常多时，它计算基数所需的空间总是固定的，而且还很小**

在 Redis 中，每个 HyperLogLog 只需要花费 12 KB 内存，就可以计算接近 2^64 个元素的基数。你看，和元素越多就越耗费内存的 Set 和 Hash 类型相比，HyperLogLog 就非常节省空间

在统计 UV 时，你可以用 PFADD 命令（用于向 HyperLogLog 中添加新元素）把访问页面的每个用户都添加到 HyperLogLog 中。

```
PFADD page1:uv user1 user2 user3 user4 user5

#返回统计结果
PFCOUNT page1:uv
```

**HyperLogLog 的统计规则是基于概率完成的，所以它给出的统计结果是有一定误差的，标准误算率是 0.81%。**

如果集合元素量达到亿级别而且不需要精确统计时，我建议你使用 HyperLogLog

## 2.5 总结

当需要进行排序统计时，List 中的元素虽然有序，但是一旦有新元素插入，原来的元素在 List 中的位置就会移动，那么，按位置读取的排序结果可能就不准确了。而 Sorted Set 本身是按照集合元素的权重排序，可以准确地按序获取结果，所以建议你优先使用它

<img src="D:\myself\springboot-example\文档\typora\images\redis20.png" alt="img" style="zoom: 25%;" />

# 三、操作系统内存机制

## 3.1 Swap

> **操作系统的内存 swap**: 操作系统里将内存数据在内存和磁盘间来回换入和换出的机制，涉及到磁盘的读写

swap 触发后影响的是 Redis 主 IO 线程，这会极大地增加 Redis 的响应时间

```
##获取redis的进程号
$ redis-cli info | grep process_id
process_id: 5332

#进入 Redis 所在机器的 /proc 目录下的该进程目录中
$ cd /proc/5332

#查看该 Redis 进程的使用情况
# Size：内存占用大小， Swap：该内存区域有多少被交换到磁盘  Size==Swap 内存已经完全被交换到磁盘了
$cat smaps | egrep '^(Swap|Size)'
Size: 584 kB
Swap: 0 kB
Size: 4 kB
Swap: 4 kB
Size: 4 kB
Swap: 0 kB
Size: 462044 kB
Swap: 462008 kB
Size: 21392 kB
Swap: 0 kB
```

增大机器内存，将其他操作大量文件的程序迁移出去

## 3.2 内存大页机制

该机制支持 2MB 大小的内存页分配，而常规的内存页分配是按 4KB 的粒度来执行的

   虽然内存大页可以给 Redis 带来内存分配方面的收益，但是，不要忘了，Redis 为了提供数据可靠性保证，需要将数据做**持久化保存**。
这个写入过程由额外的线程执行，所以，此时，Redis 主线程仍然可以接收客户端写请求。客户端的写请求可能会修改正在进行持久化的数据。在这一过程中，Redis 就会采用**写时复制机制**，也就是说，一旦有数据要被修改，Redis 并不会直接修改内存中的数据，而是将这些数据拷贝一份，然后再进行修改。

如果采用了内存大页，那么，即使客户端请求只修改 100B 的数据，Redis 也需要拷贝 2MB 的大页。相反，如果是常规内存页机制，只用拷贝 4KB。两者相比，你可以看到，当客户端请求修改或新写入数据较多时，内存大页机制将导致大量的拷贝，这就会影响 Redis 正常的访存操作，最终导致性能变慢。

```
# always：启动 no：未启动
cat /sys/kernel/mm/transparent_hugepage/enabled

#关闭内存大页机制
echo never /sys/kernel/mm/transparent_hugepage/enabled
```

## 3.3 内存分配

**当数据删除后，Redis 释放的内存空间会由内存分配器管理，并不会立即返回给操作系统。所以，操作系统仍然会记录着给 Redis 分配了大量内存**

> 内存碎片：
>
> ​	内存碎片的形成有内因和外因两个层面的原因。简单来说，内因是操作系统的内存分配机制，外因是 Redis 的负载特征。

**内存分配器的分配策略**

​	Redis 可以使用 libc、jemalloc、tcmalloc 多种内存分配器来分配内存，默认使用 `jemalloc`

​    **Jemalloc** 按照一系列固定大小划分内存空间

​		例如 8 字节、16 字节、32 字节、48 字节，…, 2KB、4KB、8KB 等。当程序申请的内存最接近某个固定值时，jemalloc 会给它分配相应大小的空间

**外因：键值对大小不一样和删改操作**

   根据jemalloc分配的是一个固定大小的内存空间，修改和删除这些键值对，会导致空间的扩容和释放。

   举例：

<img src="D:\myself\springboot-example\文档\typora\images\redis22.jpg" alt="img" style="zoom: 25%;" />

1. A、B、C、D分别保存3、1、2、4字节
2. D删除1byte
3. A修改了数据，占用4个字节，为了保证A数据的空间连续性，操作系统需要把B拷贝到别的空间，比如：D删除的1byte空间
4. C和D分别删除2byte和1byte
5. 应用E申请一个3byte的连续的内存空间，虽然有3个空闲的内存空间，但是不连续，这就形成了内存碎片

```

INFO memory
# Memory
used_memory:1073741736 #redis实际申请使用的空间
used_memory_human:1024.00M
used_memory_rss:1997159792 # 操作系统实际分配的内存空间大小包括碎片
used_memory_rss_human:1.86G
…
mem_fragmentation_ratio:1.86 #内存碎片率

mem_fragmentation_ratio = used_memory_rss/ used_memory
```

**如何清理内存碎片？**

1. 重启 Redis 实例
2. 从 4.0-RC3 版本以后，Redis 自身提供了一种内存碎片自动清理的方法

​     **碎片清理是有代价的**，操作系统需要把多份数据拷贝到新位置，把原有空间释放出来，这会带来时间开销。因为 Redis 是单线程，在数据拷贝时，Redis 只能等着，这就导致 Redis 无法及时处理请求，性能就会降低

    ```
# 启用自动内存碎片清理
config set activedefrag yes

# 下面两个条件都满足时开启自动清理
# 内存碎片的字节数达到 100MB 
active-defrag-ignore-bytes 100mb
#内存碎片空间占操作系统分配给 Redis 的总空间比例达到 10%
active-defrag-threshold-lower 10
    ```

为了尽可能减少碎片清理对 Redis 正常请求处理的影响，自动内存碎片清理功能在执行时，还会监控清理操作占用的 CPU 时间，而且还设置了两个参数，分别用于控制清理操作占用的 CPU 时间比例的上、下限，既保证清理工作能正常进行，又避免了降低 Redis 性能

```
# 自动清理过程所用 CPU 时间的比例不低于 25%，保证清理能正常开展
active-defrag-cycle-min 25
# 自动清理过程所用 CPU 时间的比例不高于 75%，一旦超过，就停止清理，从而避免在清理时，大量的内存拷贝阻塞 Redis，导致响应延迟升高。
active-defrag-cycle-max 75：
```











